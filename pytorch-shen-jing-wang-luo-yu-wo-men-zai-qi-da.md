# Pytorch, ç¥ç»ç½‘ç»œä¸æˆ‘ä»¬åœ¨æœŸå¾…ä»€ä¹ˆ   
## Section 01ï¼šåŸºè°ƒ   
1. é™ä½é¢„æœŸï¼Œæœ¬ç« ä¸æ˜¯ä½ è·å¾—ç¥ç»ç½‘ç»œç›¸å…³çŸ¥è¯†çš„åœ°ç‚¹ã€‚å®é™…ä¸Šï¼Œè¿™ä¸ªé¢†åŸŸå‘å±•çš„å¦‚æ­¤è¿…çŒ›ï¼Œ3~5å¹´ä¹‹å‰çš„è®ºæ–‡å°±è¢«æ‰“ä¸Šâ€ä¸Šå¤â€œæ ‡ç­¾ï¼Œåƒæˆ‘ç­‰åªæ˜¯å€Ÿç”¨æ¨¡å‹åœ¨æœ¬é¢†åŸŸè¿›è¡Œåº”ç”¨çš„åœºæ™¯ä¸‹ï¼Œæ˜¯å¦æœ‰èµ„æ ¼åšä»»ä½•åˆ¤æ–­æœ¬èº«å°±å­˜ç–‘ã€‚   
2. ä¸ªäººèƒ½æ”¯æŒçš„ç½‘ç»œè§„æ¨¡è™½ç„¶æœ‰é™ï¼Œä½†åŸºäºé¢†åŸŸçŸ¥è¯†çš„æ‹†åˆ†ç›®å‰è¯„ä¼°ä¹Ÿè¶³å¤Ÿåº”ç”¨å‡ºå½©ã€‚æ‰€ä»¥ä¹Ÿä¸ä¸€å®šè¦å¦„è‡ªè²è–„ï¼Œè§‰å¾—å·²ç»æ— èƒ½ä¸ºåŠ›ã€‚ç›¸åçš„ï¼Œå¤§æ¨¡å‹çœŸçš„ç¬¬ä¸€æ¬¡è®©æˆ‘èƒ½å¦‚æ­¤é«˜æ•ˆçš„åœ¨è¿™ä¸ªç»†åˆ†é¢†åŸŸåšçš„æ›´å¤šã€‚   
3. GPUç®—çš„å¿«æ˜¯äº‹å®ï¼Œä½†é‚£æ˜¯ä½ èŠ±é’±ä¹°å®ƒçš„é‚£ä¸€åˆ»å°±é¢„æœŸåˆ°çš„äº‹æƒ…ã€‚å¯¹äºåº”ç”¨GPUçš„å·¥å…·ä»·å€¼çš„é¢„æœŸ(å¿«é€Ÿçš„å‰æä¸‹ï¼Œå°½é‡çš„å‡†ç¡®ï¼Œæ›´ä¼˜çš„ç»“æœä¸æ›´ä¾¿æ·çš„æ¨¡å—è°ƒç”¨)å´æ¥è‡ªäºç¨‹åºå‘˜å’Œå·¥å…·ä½¿ç”¨è€…çš„é¢†åŸŸçŸ¥è¯†ã€‚   
   
## Section 02ï¼šå¯èƒ½å­˜åœ¨äº‰è®®çš„ä¸ªäººåˆ¤æ–­   
éä¸“èŒè¯¥é¢†åŸŸå­¦æœ¯ç ”ç©¶ï¼Œåº”ç”¨æ¨¡å‹çš„é«˜æŠ•å…¥äº§å‡ºæ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ   
1. å®è§‚å…¥æ‰‹ï¼Œå¾®è§‚æ”¾å¼ƒï¼šå°†é‡åŒ–çš„é—®é¢˜å»å¥—æœºå™¨å­¦ä¹ åˆ’åˆ†å¥½çš„åœºæ™¯ï¼Œæ¯”å¦‚æ˜¯ä¸ªå›å½’é—®é¢˜è¿˜æ˜¯ä¸ªåˆ†ç±»é—®é¢˜ï¼Ÿæ˜¯å¦é‡‡ç”¨å¢å¼ºå­¦ä¹ çš„æ¡†æ¶ï¼Ÿæ¨¡å‹å±‚é¢é€‰æ‹©å“ªç§æ›´åˆç†ï¼Ÿè¿™äº›å¯èƒ½éƒ½å·²ç»ç›¸å½“æŒ‘æˆ˜ï¼Œå°±ä¸è¦å»æ­»ç£•æ˜¯ä¸æ˜¯è‡ªå·±å‘æ˜çš„æœ‰æ¯”adamæ›´ç‰›é€¼çš„optimizerã€‚   
2. é‡åŒ–é¢†åŸŸåº”ç”¨æ—¶ï¼Œè¶Šè¿œç¦»å›´ç»•ä»·æ ¼çš„ç‚¹å€¼é¢„æµ‹å¯èƒ½é˜»åŠ›è¶Šå°ã€‚è¿™ä¸é‡‘èåˆ†æçš„é¢†åŸŸçŸ¥è¯†æœ‰å…³ï¼Œæ‰€ä»¥ä¹Ÿæ˜¯è§ä»è§æ™ºã€‚   
3. ä¸“æ³¨é¢†åŸŸçŸ¥è¯†æ·±å…¥ï¼Œå¾®å°çš„æ¨¡å‹åªè§£å†³å•ä¸€ç®€å•é—®é¢˜ã€‚ç¥ç»ç½‘ç»œæ¨¡å‹æœ‰ä¸ªç‰¹ç‚¹ï¼Œå¾ˆå¤šåº”ç”¨åœºæ™¯å®é™…ä¸Šæ¨¡å‹è‡ªèº«çš„å˜åŒ–å¹¶ä¸å¤§ï¼Œåå€’æ˜¯æ•°æ®å±‚é¢çš„å¤„ç†èµ‹äºˆäº†å…¶ç›¸å…³çš„æ„ä¹‰ã€‚è¿™å°±å¼•å‡ºäº†ä¸¤ä¸ªæ€è€ƒæ–¹å‘ï¼š   
    > å¾…å¤„ç†çš„é—®é¢˜å›ºå®šï¼Œç”¨ä»€ä¹ˆæ¨¡å‹å·¥å…·åˆé€‚   

    ä¸¾ä¾‹æ¥è¯´ï¼Œä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’é—®é¢˜ï¼Œå…¶å®å¯ä»¥ç”¨æ ‡å‡†çš„FCæ¥å¤„ç†ï¼Œä¹Ÿå¯ä»¥ç”¨RLæ–¹å¼æ¥å¤„ç†ï¼Œä»¥ä¸‹ä¸ºä»£ç ä¾‹å­ã€‚æ¯”è¾ƒæ¥çœ‹ï¼ŒRLçš„å…·ä½“æ¨¡å‹ä¸FCéƒ½æ˜¯ä¸€è‡´çš„ï¼Œä»…ä»…æ˜¯è®­ç»ƒæ›´æ–°æœºåˆ¶ä¸­å­˜åœ¨ä¸åŒã€‚ä»ç»“æœæ¥çœ‹ï¼Œä¸¤è¾¹éƒ½å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜, æ™®é€šçš„å…¨è¿æ¥ç½‘ç»œï¼ˆFCï¼‰æŸå¤±ä¸‹é™å¾ˆå¿«ã€‚å¼ºåŒ–å­¦ä¹ ä¸­çš„ç½‘ç»œåˆ™éœ€è¦é€šè¿‡ç¯å¢ƒçš„åé¦ˆé€æ­¥è°ƒæ•´ï¼Œå­¦ä¹ è¿‡ç¨‹è¾ƒä¸ºç¼“æ…¢ã€‚   
    |                                                                                                                                                                                                                                                                                                                                                                                           FC model |                                                                                                                                                                                                                                                                                                                                                                                          DQN model |
    |:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | Epoch 0, Loss: 132.7027587890625 Epoch 100, Loss: 0.015572714619338512 Epoch 200, Loss: 0.00017316047160420567 Epoch 300, Loss: 7.045352685963735e-05 Epoch 400, Loss: 4.023780275019817e-05 Epoch 500, Loss: 2.6890053050010465e-05 Epoch 600, Loss: 1.8399234249955043e-05 Epoch 700, Loss: 1.1512761375342961e-05 Epoch 800, Loss: 6.998458047746681e-06 Epoch 900, Loss: 4.500810973695479e-06 | Epoch 0, Loss: 144.50608825683594 Epoch 100, Loss: 0.027036404237151146 Epoch 200, Loss: 0.0019762783776968718 Epoch 300, Loss: 0.00021854773513041437 Epoch 400, Loss: 8.594315295340493e-05 Epoch 500, Loss: 5.9787387726828456e-05 Epoch 600, Loss: 4.43860626546666e-05 Epoch 700, Loss: 3.4103391953976825e-05 Epoch 800, Loss: 2.7173697162652388e-05 Epoch 900, Loss: 2.242411756014917e-05 |

    ```
# DQN.py model

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
x = np.linspace(-10, 10, 100).reshape(-1, 1)
y = 2 * x + 3  # ç›®æ ‡å‡½æ•°

# è½¬æ¢ä¸º PyTorch å¼ é‡
x_train = torch.FloatTensor(x)
y_train = torch.FloatTensor(y)

# å®šä¹‰ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç½‘ç»œ
class SimpleFC(nn.Module):
    def __init__(self):
        super(SimpleFC, self).__init__()
        self.fc1 = nn.Linear(1, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 1)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = SimpleFC()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# è®­ç»ƒç½‘ç»œ
epochs = 1000
losses = []
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    output = model(x_train)
    loss = criterion(output, y_train)
    loss.backward()
    optimizer.step()
    losses.append(loss.item())
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")

# å¯è§†åŒ–ç»“æœ
model.eval()
predicted = model(x_train).detach().numpy()

plt.plot(x, y, label='True')
plt.plot(x, predicted, label='Predicted')
plt.legend()
plt.show()
```
    ```
# FC.py model

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
x = np.linspace(-10, 10, 100).reshape(-1, 1)
y = 2 * x + 3  # ç›®æ ‡å‡½æ•°

# è½¬æ¢ä¸º PyTorch å¼ é‡
x_train = torch.FloatTensor(x)
y_train = torch.FloatTensor(y)

# å®šä¹‰ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç½‘ç»œ
class SimpleFC(nn.Module):
    def __init__(self):
        super(SimpleFC, self).__init__()
        self.fc1 = nn.Linear(1, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 1)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = SimpleFC()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# è®­ç»ƒç½‘ç»œ
epochs = 1000
losses = []
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    output = model(x_train)
    loss = criterion(output, y_train)
    loss.backward()
    optimizer.step()
    losses.append(loss.item())
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")

# å¯è§†åŒ–ç»“æœ
model.eval()
predicted = model(x_train).detach().numpy()

plt.plot(x, y, label='True')
plt.plot(x, predicted, label='Predicted')
plt.legend()
plt.show()
```
    > æ¨¡å‹å·¥å…·å›ºå®šï¼Œæ€ä¹ˆè°ƒæ•´å¾…å¤„ç†çš„é—®é¢˜   

    1-D CNNåœ¨é‡‘èæ—¶åºæ•°æ®çš„åº”ç”¨ã€‚æ­¤å¤„çº¯ç²¹æ˜¯å› ä¸ºæƒ³è¦ä½¿ç”¨1-D CNNï¼Œæ‰€ä»¥ç”¨è¿‡å»çš„æ—¶åºæ•°æ®æ¥å……å½“è¾“å…¥çš„ä¿¡æ¯ã€‚è¿™é‡Œçš„é—®é¢˜åœ¨äºä¸¤ä¸ªæ–¹é¢ï¼š   
    - Conv1Dè‡ªèº«æ˜¯ä¸çŸ¥é“æ•°æ®è‡ªèº«çš„åºåˆ—å±æ€§ã€‚è¿™å®é™…ä¸Šæ˜¯ä½¿ç”¨è€…äººå·¥èµ‹äºˆçš„æ•°æ®æŒ–æ˜æ–¹å‘ã€‚å…¶å®å¦‚æœå…·å¤‡ï¼Œç”¨å…¶ä»–çš„ä¿¡æ¯å¯èƒ½æ›´ä¸ºåˆç†ã€‚ä½†ä¸¾ä¾‹å­å°±ä¸è¦å¤ªçº ç»“äº†ã€‚   
    - pytorchå®é™…ä¸Šåœ¨ç½‘ç»œå±‚çº§ä¸Šç»™äº†å¾ˆå¤šé¢„è®¾çš„layerï¼Œå¦‚æœå­˜åœ¨å°±ç›´æ¥ç”¨å°±å¥½ï¼Œå®ƒå¤§æ¦‚ç‡æ¯”ä½ è‡ªå·±åšçš„è‡ªå®šä¹‰ç›¸å…³å®ç°æœ‰æ›´å¥½çš„ä¼˜åŒ–è¡¨ç°ã€‚å¦‚æœæ²¡æœ‰å†è€ƒè™‘æ˜¯ä¸æ˜¯è¦è‡ªå®šä¹‰å®ç°ç›¸å…³æœºåˆ¶ã€‚   
        - å®é™…ä¸Šå¦‚æœçœŸçš„æ²¡æœ‰ï¼Œç¬¬ä¸€ä¸ªç›´è§‰å°±æ˜¯â€æ˜¯ä¸æ˜¯è¿™ä¸ªé¢†åŸŸæœ‰å…¶ä»–çš„æ›´å¥½çš„æ¨¡å‹å‡ºç°äº†ï¼Ÿâ€è¿™ä¸ªé¢†åŸŸçš„çŸ¥è¯†æ›´æ–°é€Ÿåº¦å¤ªå¿«äº†ï¼Œå°±å¥½åƒRNNæ¨¡å‹åœ¨å¤§èŒƒå›´çš„è¢«self-attentionæ›¿ä»£ä¸€æ ·ï¼Œæˆ‘ä»¬è¿™ç§éè¯¥é¢†åŸŸä¸“èŒç ”ç©¶å‘˜æœ‰æ‰€é—æ¼å†æ­£å¸¸ä¸è¿‡äº†ã€‚   
   
    å”¯ä¸€éœ€è¦ç•™æ„çš„å°±æ˜¯ä¸åŒç±»å‹çš„ç½‘ç»œè¾“å…¥çš„æ•°æ®çš„å·®å¼‚   
    â€‡â€‡ä¸¾ä¾‹æ¥è¯´ï¼š   
    â€‡â€‡RNNçš„æ¨¡å‹inputå¯èƒ½æ˜¯**(sequence\_length, batch\_size, num\_features)** or **(batch\_size, sequence\_length, num\_features)**   
    â€‡â€‡FCçš„æ¨¡å‹inputå¯èƒ½æ˜¯**(batch\_size, num\_features)**   
    â€‡â€‡Conv1dçš„æ¨¡å‹inputå¯èƒ½æ˜¯**(batch\_size, in\_channels, seq\_length)**   
    â€‡â€‡Conv1dæ¥FCè¦å±•å¼€å…¶è¾“å‡ºä»¥é€‚é…FCçš„è¾“å…¥é“ç†ç±»ä¼¼ï¼Œè¿™ä¸ªåœ°æ–¹ç•™æ„ä¸€ä¸‹å®¹æ˜“å‡ºé—®é¢˜ã€‚   
    â€‡â€‡```
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# 1. ç”Ÿæˆç¤ºä¾‹æ•°æ®
def generate_data(seq_length, n_samples):
    np.random.seed(42)
    x = np.linspace(0, 100, n_samples)
    y = np.sin(x) + np.random.normal(0, 0.1, n_samples)  # æ·»åŠ å™ªå£°çš„ä»·æ ¼æ•°æ®
    data = []
    for i in range(len(x) - seq_length):
        data.append(y[i:i + seq_length])
    return np.array(data), y[seq_length:]

seq_length = 5
n_samples = 200
X, y = generate_data(seq_length, n_samples)

# 2. æ•°æ®é¢„å¤„ç†å¹¶è½¬æ¢ä¸ºPyTorchå¼ é‡
X_train = torch.FloatTensor(X).unsqueeze(1)  # (batch_size, 1, seq_length)
y_train = torch.FloatTensor(y).unsqueeze(-1)  # (batch_size, 1)

# 3. å®šä¹‰1D CNNæ¨¡å‹
class CNN1DTimeSeries(nn.Module):
    def __init__(self):
        super(CNN1DTimeSeries, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=2)  # Conv1Då±‚
        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=2)
        
        # è®¡ç®—å…¨è¿æ¥å±‚è¾“å…¥å¤§å°
        self.flatten_size = self._get_conv_output_shape()

        self.fc1 = nn.Linear(self.flatten_size, 50)  # å…¨è¿æ¥å±‚1
        self.fc2 = nn.Linear(50, 1)  # æœ€ç»ˆè¾“å‡ºä¸ºå•ä¸€ä»·æ ¼é¢„æµ‹

    def _get_conv_output_shape(self):
        # ç”¨éšæœºå¼ é‡æ¨å¯¼å·ç§¯å±‚çš„è¾“å‡ºå½¢çŠ¶
        with torch.no_grad():
            x = torch.zeros(1, 1, seq_length)  # (batch_size=1, in_channels=1, seq_length)
            x = torch.relu(self.conv1(x))
            x = torch.relu(self.conv2(x))
            return x.numel()  # è®¡ç®—å±•å¹³åçš„å…ƒç´ æ•°é‡

    def forward(self, x):
        x = torch.relu(self.conv1(x))  # å·ç§¯1
        x = torch.relu(self.conv2(x))  # å·ç§¯2
        x = x.view(x.size(0), -1)  # å±•å¹³
        x = torch.relu(self.fc1(x))  # å…¨è¿æ¥å±‚1
        return self.fc2(x)  # è¾“å‡ºå±‚

# 4. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
model = CNN1DTimeSeries()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 5. è®­ç»ƒæ¨¡å‹
epochs = 100
losses = []
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    
    output = model(X_train)
    loss = criterion(output, y_train)
    loss.backward()
    optimizer.step()
    
    losses.append(loss.item())
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item()}')

# 6. ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
plt.plot(losses)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

# 7. æµ‹è¯•æ¨¡å‹å¹¶å¯è§†åŒ–ç»“æœ
model.eval()
predicted = model(X_train).detach().numpy()

# 8. å¯è§†åŒ–å®é™…ä»·æ ¼ä¸é¢„æµ‹ä»·æ ¼
plt.plot(y, label='True Prices')
plt.plot(predicted, label='Predicted Prices')
plt.legend()
plt.show()

# Epoch 0, Loss: 0.5234584212303162
# Epoch 10, Loss: 0.37698492407798767
# Epoch 20, Loss: 0.19763697683811188
# Epoch 30, Loss: 0.0395878367125988
# Epoch 40, Loss: 0.0300571471452713
# Epoch 50, Loss: 0.015313473530113697
# Epoch 60, Loss: 0.015903208404779434
# Epoch 70, Loss: 0.013504610396921635
# Epoch 80, Loss: 0.013377712108194828
# Epoch 90, Loss: 0.013072483241558075

```
    â€‡â€‡![æˆªå±2024-09-05 13.07.20.png](files/jie-ping-2024-09-05-13-07-20.png)    
       
    è¯´ä¸å‡ºåŸå› ï¼Œæˆ‘ä¸ªäººæ›´å–œæ¬¢GANç±»å‹çš„ç½‘ç»œï¼Œæœ‰ä¸€ç§å˜æ€çš„å¿«ä¹ï¼Œå°±æ˜¯æ²¡å¤ªæƒ³å¥½åº”ç”¨åœºæ™¯ğŸ˜…   
4. å¤§æ¨¡å‹è™½ç„¶ä¸èƒ½é¢é¢å…·å¤‡ï¼Œä½†æ˜¾ç„¶ç•™ç»™æ™®é€šäººçš„æ—¶é—´ä¸å¤šäº†ã€‚å½“è¿›è¡Œè¡¨è¾¾å¼ç”Ÿæˆé—®é¢˜çš„æ—¶å€™ï¼Œæœ¬æ–‡å®é™…ä¸Šæƒ³åšä¸€ä¸ªåŸºäºKarva Expressionçš„ç¥ç»ç½‘ç»œç”Ÿæˆçš„æ¡ˆä¾‹ä»¥å¯¹æ¯”GEPã€‚GPT-4oæœ¬èº«äº†è§£è¿™ä¸€æ¦‚å¿µï¼Œä½†æ˜¯åšçš„å¹¶ä¸å¥½ã€‚æä¾›æ–‡çŒ®å‚è€ƒåï¼Œæ˜¾ç„¶èƒ½åšçš„æ›´å¤šï¼Œç”šè‡³ä¹Ÿæå‡ºäº†ä½¿ç”¨transformerè¿›è¡Œç”Ÿæˆçš„æ–¹æ¡ˆï¼Œä½†ç”Ÿæˆè¿‡ç¨‹ä¼´éšç€é•¿æ—¶é—´çš„è¡¨è¾¾å¼è§£æå¼‚å¸¸ã€‚ç©¶å…¶åŸå› ï¼Œæ˜¯KEçš„å°¾éƒ¨çš„éç®—å­ç‰¹å¾å§‹ç»ˆè¢«å¤§æ¨¡å‹å¿½ç•¥ã€‚æˆ–è®¸å†å¤§åŠ›ä¸€ç‚¹å°±ä¼šå‡ºå¥‡è¿¹ï¼Œæ¯•ç«Ÿæˆ‘çš„Macè¿˜æ˜¯æœ‰ç‚¹æ€§èƒ½æå‡ç©ºé—´ã€‚ä½†å¯èƒ½ä¸‹ä¸€ä¸ªç‰ˆæœ¬çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œè¿™ä¸ªé—®é¢˜å°±å¯ä»¥é€šè¿‡ä¿®æ­£ç”Ÿæˆå‡½æ•°è€Œå¾—åˆ°å½»åº•è§£å†³ã€‚æ‰€ä»¥ï¼Œæˆ–è®¸æœªæ¥çœŸçš„ä¼šæˆä¸ºä¸€ä¸ªéœ€è¦å¯¹è‡ªèº«é¢†åŸŸè¶³å¤Ÿäº†è§£ï¼Œæ¶‰çŒå¹¿æ³›èƒ½å‘ç°ç—›ç‚¹å¹¶æå‡ºæŠ€æœ¯æ”¹è¿›ç—›ç‚¹çš„â€œå¤©å‘½äººâ€æ‰ä¼šå¾—ä»¥ç”Ÿå­˜å§ã€‚   
   
## Section 03ï¼šåˆç†é¢„æœŸ   
1. æˆ–è®¸è¿™ç›®å‰ä»ç„¶ä¸æ˜¯ä¸€ä¸ªå¯ä»¥å®Œå…¨è¢«å¤§æ¨¡å‹ç¢¾å‹çš„é¢†åŸŸï¼Œä½†ä¸“ä¸šé¢†åŸŸçŸ¥è¯†çš„æŠ¤åŸæ²³ç¡®å®è¶Šæ¥è¶Šå°ï¼Œä¼´éšç€è¾…åŠ©å·¥å…·çš„ä¸æ–­å®Œå–„ï¼Œåˆ†æå¸ˆçš„å·¥ä½œä»·å€¼å°†è¶Šæ¥è¶Šä½ã€‚   
2. ä¸€ä¸ªè‰¯å¥½çš„æœºåˆ¶ä¸æ–­åœ°äº§ç”Ÿç¬¦åˆè¦æ±‚çš„ç­–ç•¥å®ä¾‹ä»»åŠ¡ã€‚   
3. ç¥ç»ç½‘ç»œè¿™ä¸ªé¢†åŸŸè¿‡äºåºå¤§ï¼ŒæŠ•å…¥å¤šå°‘æ—¶é—´éƒ½ä¸å¯èƒ½å®Œå¤‡ã€‚å¦‚æœæœ‰å…¶ä»–æ¡†æ¶çº§åˆ«çš„é‡è¦äº‹é¡¹ï¼Œåº”è¯¥å…ˆå®Œæˆå…¶ä»–ï¼Œç„¶åå†è°ˆè®ºæœ¬å¤„ã€‚å½“ç„¶ï¼ŒTAçš„ä¼˜å…ˆçº§å®šä½åœ¨æˆ‘ä»¬æ‰€è¯´çš„é‡åŒ–åˆ†ææ¡†æ¶é‡Œç›®å‰å……å½“çš„å‡½æ•°æœç´¢çš„ä½œç”¨ä¸æ˜¯å”¯ä¸€å¤‡é€‰æœ‰å…³ç³»ã€‚   
   
##    
   
